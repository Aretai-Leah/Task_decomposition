{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_graph_modelling_language_using_select_speaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Planning and Task decomposition\n",
    "\n",
    "Good planning ans task decomposition enables more complex request to be completed with higher reliability. \n",
    "In this notebook, we'll explore how to implement effective and flexible planning and task decomposition.\n",
    " \n",
    "\n",
    "There are several key capabilities in use here:\n",
    "\n",
    "**JSON Mode:** This allows for very specific multi-part instructions to be passed between agents with a reliable fixed structure.\n",
    "\n",
    "**Function calling.** This universalises the connectivity between the separate action space capabilites, provding a easily replicated template, separates the action space from the planning and allows for (pseudo) parralel processing where multiple steps are non sequential.\n",
    "\n",
    "\n",
    "This example uses the following Agent strucutre.\n",
    "\n",
    "\n",
    "![Speaking Transitions](planner_article_speaker_graph.jpg)\n",
    "\n",
    "\n",
    "Note there are three groups of Agents. \n",
    "\n",
    "* **Planning group.** This is the top most group, which the user interacts with and generates a high-level plan to be followed by other agents.\n",
    "\n",
    "* **Web Group.** Nested under the planning group within a function call. This group facilitates web search functions using the Autogen Web Surf agent. \n",
    "\n",
    "* **File Group.** This group is also separated by a function call. It facilitates read and write requests to the file system. For demo purposes, these are limted to read and write to a single file. Note that this group mirrors the structure of the planning group.\n",
    "\n",
    "\n",
    "Documentation on JSON mode from Open AI can be found [here](https://platform.openai.com/docs/guides/text-generation/json-mode) \n",
    "\n",
    "A simple implementation of JSON mode in Autogen can be found [here] (the previous JSON example.)\n",
    "\n",
    "Benefits\n",
    "- This contribution provides a strucutre for handling planning and task decomposition within a group chat \n",
    "\n",
    "\n",
    "## Requirements\n",
    "JSON mode is a feature of OpenAI API, however strong models (such as claude 3 Opus), can generate appropriate json as well.\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Some extra dependencies are needed for this notebook, which can be installed via pip:\n",
    "\n",
    "```bash\n",
    "pip install pyautogen[retrievechat] flaml[automl]\n",
    "pip install pathlib\n",
    "pip install typing\n",
    "pip install json\n",
    "\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](/docs/installation/).\n",
    ":::\n",
    "````\n",
    " \n",
    "In Your OAI_CONFIG_LIST file, you must have three configs, \n",
    "one with           \"response_format\": { \"type\": \"text\" } \n",
    "and the other with \"response_format\": { \"type\": \"json_object\" }\n",
    "and lastly a GPT 3.5 config for the web Surf agent summariser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install \"pyautogen>=0.2.3\"\n",
    "# %pip install \"pyautogen>=0.2.3\"\n",
    "\n",
    "\n",
    "[\n",
    "    {\"model\": \"gpt-4-turbo-preview\", \"api_key\": \"key go here\", \"response_format\": {\"type\": \"text\"}},\n",
    "    {\"model\": \"gpt-4-0125-preview\", \"api_key\": \"key go here\", \"response_format\": {\"type\": \"json_object\"}},\n",
    "    {\"model\": \"gpt-3.5-turbo-16k\", \"api_key\": \"key go here\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "import os\n",
    "import json\n",
    "from autogen.agentchat.groupchat import GroupChat\n",
    "from autogen.agentchat.assistant_agent import AssistantAgent\n",
    "from autogen.agentchat import UserProxyAgent\n",
    "from typing import List, Dict\n",
    "from autogen.agentchat.assistant_agent import AssistantAgent  # noqa E402\n",
    "from autogen.agentchat.contrib.web_surfer import WebSurferAgent  # noqa: E402\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "we Need to set a few different Configs for this to work, JSON and text configs as well as a GTP 3.5 config for the Web Surf agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list_text = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-turbo-preview\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "config_list_json = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-0125-preview\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "manager_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": autogen.config_list_from_json(\n",
    "        \"OAI_CONFIG_LIST\",\n",
    "        filter_dict={\"model\": [\"gpt-4-turbo-preview\"]},\n",
    "    ),\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "summarizer_llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": 44,  # change the seed for different trials\n",
    "    \"config_list\": autogen.config_list_from_json(\n",
    "        \"OAI_CONFIG_LIST\",\n",
    "        filter_dict={\"model\": [\"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-16k-0613\", \"gpt-3.5-turbo-16k\"]},\n",
    "    ),\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "llm_config = {\"config_list\": config_list_text, \"seed\": 42}\n",
    "\n",
    "w_llm_config = {\"config_list\": config_list_text, \"seed\": 42}\n",
    "\n",
    "\n",
    "f_llm_config = {\"config_list\": config_list_text, \"seed\": 42}\n",
    "\n",
    "\n",
    "json_llm_config = {\"config_list\": config_list_json, \"seed\": 42}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Configuration \n",
    "\n",
    "As we are using the Autogen Web Surg agent, we need a bing api key. \n",
    "We also need to specify a file for the File handling agents to interact with. This strucutre can manage very broad file and directroy handling, however for the purposes of the demo, we are limiting this to a single specified file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = Path(\"absolute path goes here\")\n",
    "\n",
    "os.environ[\"BING_API_KEY\"] = \"your bing api key goes here\"\n",
    "\n",
    "bing_api_key = os.environ[\"BING_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(autogen.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the task\n",
    "\n",
    "The task for our JSON example is to answer the question: \"Who won the last F1 race in mebourne Australia? Save results to disk.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_user_request = \"\"\"Who won the last F1 race in mebourne Australia? Save results to disk\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Task Decomposition Agent\n",
    "\n",
    "The Task decomposition Agent is the lynchpin upon which this capability rests.  The prompt must be carefully written to output a useful list of tasks that other agents can act upon. \n",
    "\n",
    "We tell the agent:\n",
    "* who they are\n",
    "* what their job is\n",
    "* what functions the group has access to\n",
    "* what the output strucutre must be\n",
    "\n",
    "\n",
    "We also provide multiple examples of task workflows. In this case we can have isolated tasks, or task that require input from a previous task. \n",
    "It is also neccessary to give the task decomposition a limit to how much they should break down the task. The sub processes will break down the task further, so we don't need to break down the task to individual actions. This model allows for heirarchical task decomposition which can enable an agent system to tackle complex tasks with simple instructions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_agent = AssistantAgent(\n",
    "    name=\"decomp_agent\",\n",
    "    system_message=\"\"\"You are a task decomposition agent. You are part of a team of AI agents who's collective job is to find information on the internet and then save the result to disk.\n",
    "                   Your role is to assess the request posed by the user and break the request up into function calls that will initiate action to fulfil that decision. \n",
    "   \n",
    "                   The decision may be very simple. it may require no more than call the speech agent. Be careful not to overcomplicate the task.  \n",
    "                   You must parse the requests of the user and identify the main functions required to complete the task. The available functions are:\n",
    "\n",
    "                    - Web interactions. initiated by calling main_process_web\n",
    "                    - File reading and writing. initiated by calling main_process_file_ops\n",
    "                    - Speech generation. handled by calling the speech_agent\n",
    "\n",
    "                    When decomposing tasks, focus on identifying the high-level functions needed and provide a clear task description for each function. Avoid breaking down the specific steps required within each function, as that should be handled by the respective specialized agents.\n",
    "                    However, where we need to collect and save results for secondary steps, make sure that the discrete results are saved to disk. This will likely involve interleaving web function calls with file ops function calls.\n",
    "\n",
    "                    Use the following JSON format for the task decomposition:\n",
    "\n",
    "                    {\n",
    "                        \"task_decomposition\": {\n",
    "                            \"inbound_request\": \"The overall objective\",\n",
    "                            \"function_1\": {\n",
    "                                \"name\": \"The name of the function (e.g.,  main_process_web,  main_process_file_ops)\",\n",
    "                                \"task\": \"A clear task description for the function. including any neccessary context, quotes or verbose details.\"\n",
    "                            },\n",
    "                            \"function_2\": {\n",
    "                                \"name\": \"The name of the function  (e.g.,  main_process_web,  main_process_file_ops)\",\n",
    "                                \"task\": \"A clear task description for the function  including any neccessary context, quotes or verbose details.\"\n",
    "                            },\n",
    "                            ...\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    If a function requires input from another function, specify the dependency using the 'input_from' field:\n",
    "\n",
    "                    {\n",
    "                        \"task_decomposition\": {\n",
    "                            \"inbound_request\": \"The overall objective\",\n",
    "                            \"function_1\": {\n",
    "                                \"name\": \"The name of the function  (e.g.,  main_process_web,  main_process_file_ops)\",\n",
    "                                \"task\": \"A clear task description for the function  including any neccessary context, quotes or verbose details.\"\n",
    "                            },\n",
    "                            \"function_2\": {\n",
    "                                \"name\": \"The name of the function  (e.g.,  main_process_web,  main_process_file_ops)\",\n",
    "                                \"task\": \"A clear task description for the function  including any neccessary context, quotes or verbose details.\",\n",
    "                                \"input_from\": \"function_1\"\n",
    "                            },\n",
    "                            ...\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    Focus on identifying the necessary functions, providing clear task descriptions, and optimizing the task decomposition by combining consecutive function calls of the same type. Leave the specific task breakdown and execution to the specialized agents.\n",
    "                    \"\"\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=json_llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Agent and Executive proxy\n",
    "\n",
    "The job of the task_decomp agent is to provide a list of actions. The Exec_agent and Exec_proxy take that task list and then put it into action. This 3 agent structure, task_decomp, exec agent,  exec proxy is repeatable and very flexible for handling complex tasks. \n",
    "\n",
    "The Exec_agent constructs the function calls based on the task as described by the decomp agent.\n",
    "The Exec_proxy calls the function. \n",
    "\n",
    "\n",
    "We tell the exec_agent:\n",
    "* who they are\n",
    "* what their job is\n",
    "* how to reads the decomp_agent's output\n",
    "* when to use each function\n",
    "* how to optimise the function calls \n",
    "* what the data requirements are for each call\n",
    "\n",
    "Effectively, the exec_agent is perfroming ad-hoc API calls, and their system prompt is the technical documentation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exec_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_agent = AssistantAgent(\n",
    "    name=\"exec_agent\",\n",
    "    system_message=\"\"\"\n",
    "                You are an executive function agent responsible for managing and coordinating various sub-processes within an AI Web Research system. Your primary role is to receive high-level requests from the decomp_agent, analyze them, and delegate the appropriate tasks to the relevant sub-processes.\n",
    "\n",
    "                When you receive a request from the decomp_agent, your job is to:\n",
    "                1. Carefully analyze the task decomposition provided by the decomp_agent.\n",
    "                2. Identify the dependencies between the functions, ensuring that the output of one function is available as input to another function when required.\n",
    "                3. Combine function calls of the same type (e.g., web interactions) to optimize the process and reduce redundant calls.\n",
    "                4. Execute the functions in the correct order, considering the dependencies and optimizations.\n",
    "                5. Collect the results from each function and pass them as input to the subsequent functions as needed.\n",
    "                6. Compile the final results and return them to the decomp_agent or the appropriate agent for further processing.\n",
    "\n",
    "                To combine function calls of the same type, follow these guidelines:\n",
    "                - If multiple web interactions are requested consecutively, combine them into a single web interaction request.\n",
    "                - If multiple file handling operations are requested consecutively, combine them into a single file handling request.\n",
    "                - If multiple goal handling operations are requested consecutively, combine them into a single goal handling request.\n",
    "                - Ensure that the combined request clearly specifies the individual tasks and their dependencies.\n",
    "\n",
    "                When executing the functions, use the appropriate tool calls:\n",
    "                - For web interactions, use the 'main_process_web' tool with the combined web request as the argument.\n",
    "                - For file handling operations, use the 'main_process_file_ops' tool with the combined file handling request as the argument.\n",
    "                - Pass all necessary context and results from previous functions as input to the subsequent functions.\n",
    "                - There is no way for the functions to access information except what you pass them. If a function is to search for a string, you must pass the whole string. If a function is to save information, you must pass all that information. There is no other data path. \n",
    "\n",
    "                Your role is crucial in ensuring efficient execution of the tasks and proper flow of information between the sub-processes. Strive to optimize the process, minimize redundant calls, and ensure the correct order of execution based on the dependencies.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exec_proxy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_proxy = UserProxyAgent(\n",
    "    name=\"exec_proxy\",  # system_message= system_messages.judge_systemmessage,\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    ")  # IMPORTANT: set to True to run code in docker, recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termination Message \n",
    "\n",
    "We need several different termination messages in this strucutre, as the sub groups need to be able to end without any further interaction, but without also terminating the parent group. \n",
    "This is the outer termination message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_termination_msg(content) -> bool:\n",
    "    have_content = content.get(\"content\", None) is not None\n",
    "    if have_content and \"TERMINATE\" in content[\"content\"]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Agents\n",
    "\n",
    "User proxy is the standard user proxy and the speech agent summarises the actions taken by the group to form a coherent response. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_agent = AssistantAgent(\n",
    "    name=\"speech_agent\",\n",
    "    system_message=\"summarise the result of the groupchat in an friendly conversational manner. Be modest, and speak as you imagine a professional colleage would to another colleague around the watercooler\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    system_message=\"Terminator admin. Reply in JSON\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=is_termination_msg,\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions\n",
    "\n",
    "We have two functions. \n",
    "1. Handling file interactions (only read and write to a single file in this example)\n",
    "2. Web Searching (using the autogen Web_surf Agent)\n",
    "\n",
    "These are simple examples, but the strucutre is very reusable.  The functions are registered to the exec_proxy and assigned to the exec_agent to call. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@exec_proxy.register_for_execution()\n",
    "@exec_agent.register_for_llm(description=\"Internet and web browser tools\")\n",
    "def main_process_web(web_message: str) -> dict:\n",
    "    chat_result = process_web(web_message)\n",
    "\n",
    "    # Iterate through the chat_history in reverse order\n",
    "    for message in reversed(chat_result.chat_history):\n",
    "        if message[\"name\"] == \"webresponse_agent\":\n",
    "            response_content = message[\"content\"]\n",
    "            break\n",
    "    else:\n",
    "        response_content = \"No response found from webresponse_agent\"\n",
    "\n",
    "    return response_content\n",
    "\n",
    "\n",
    "@exec_proxy.register_for_execution()\n",
    "@exec_agent.register_for_llm(description=\"File operations tools\")\n",
    "def main_process_file_ops(file_message: str, context: str) -> dict:\n",
    "    # Create a dictionary containing the file message and context\n",
    "    file_request = {\"file_message\": file_message, \"context\": context}\n",
    "\n",
    "    # Convert the file request to JSON string\n",
    "    file_request_json = json.dumps(file_request)\n",
    "\n",
    "    chat_result = process_file_ops(file_request_json)\n",
    "\n",
    "    # Iterate through the chat_history in reverse order\n",
    "    for message in reversed(chat_result.chat_history):\n",
    "        if message[\"name\"] == \"fileresponse_agent\":\n",
    "            response_content = message[\"content\"]\n",
    "            break\n",
    "    else:\n",
    "        response_content = \"No response found from fileresponse_agent\"\n",
    "\n",
    "    return response_content\n",
    "\n",
    "\n",
    "exec_agent.llm_config[\"tools\"]\n",
    "\n",
    "assert exec_proxy.function_map[\"main_process_file_ops\"]._origin == main_process_file_ops\n",
    "assert exec_proxy.function_map[\"main_process_web\"]._origin == main_process_web"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Process\n",
    "\n",
    "The Web process forms its own inner group chat. The following elements are required: \n",
    "\n",
    "* A function to receive the message from the outer group and pass it to the chat manager\n",
    "* A distinct termination message\n",
    "* A userproxy to receive the inital message and to receive to termination message\n",
    "* the Web Surf agent (or other web tools much as Multi-On or Open interpreter)\n",
    "* response agent. To summarise results and pass the termination message\n",
    "* separate groupchat, manager and allowed transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminus_W_msg(content) -> bool:\n",
    "    have_content = content.get(\"content\", None) is not None\n",
    "    if have_content and \"TERMINUS_W\" in content[\"content\"]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "surf_agent = WebSurferAgent(\n",
    "    name=\"surf_agent\",\n",
    "    system_message=\"\"\"You are a helpful web surfing agent\"\"\",  # system_messages.multionagent_systemmessage,\n",
    "    summarizer_llm_config=summarizer_llm_config,\n",
    "    browser_config={\"viewport_size\": 4096, \"bing_api_key\": bing_api_key},\n",
    "    llm_config=w_llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    description=\"Call this Agent if you need to access the internet.\",\n",
    ")\n",
    "\n",
    "web_userproxy = UserProxyAgent(\n",
    "    name=\"web_userproxy\",\n",
    "    system_message=\"Terminator admin. Reply in JSON\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=is_terminus_W_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "\n",
    "webresponse_agent = AssistantAgent(\n",
    "    name=\"webresponse_agent\",\n",
    "    system_message=\"\"\"You are the web response agent. Your job is to collate the results of the web search so that it can be used by the next team. Ensure that you are comprehensive, as you are not certian what they need. Always end with: TERMINUS_W. This tell the sytem to proceed to the next step.\"\"\",\n",
    "    llm_config=w_llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "\n",
    "web_allowed_transitions = {\n",
    "    web_userproxy: [surf_agent],\n",
    "    surf_agent: [webresponse_agent],\n",
    "    webresponse_agent: [web_userproxy],\n",
    "}\n",
    "\n",
    "web_group_chat = GroupChat(\n",
    "    agents=(web_userproxy, surf_agent, webresponse_agent),\n",
    "    messages=[],\n",
    "    allowed_or_disallowed_speaker_transitions=web_allowed_transitions,\n",
    "    speaker_transitions_type=\"allowed\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create the manager\n",
    "web_manager = autogen.GroupChatManager(\n",
    "    groupchat=web_group_chat,\n",
    "    llm_config=manager_config,\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=is_terminus_W_msg,\n",
    ")\n",
    "\n",
    "\n",
    "def process_web(web_request):\n",
    "    user_request = json.dumps(web_request)\n",
    "\n",
    "    chat_result = web_userproxy.initiate_chat(\n",
    "        web_manager,\n",
    "        message=(user_request),\n",
    "    )\n",
    "\n",
    "    return chat_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Process\n",
    "\n",
    "The File process works more like the main planning grouchat than the Web process. \n",
    "It has: \n",
    "* Its own task decomposition, 'file' agent and proxy\n",
    "* its own functions to call\n",
    "\n",
    "This demonstrates how task decomposition using functions can be nested heirarchically, breaking down task are far as they need to before passing completed actions back up the chain. \n",
    "\n",
    "Specifically in this example, the functions are a simple read function and write function tied to a specific file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminus_F_msg(content) -> bool:\n",
    "    have_content = content.get(\"content\", None) is not None\n",
    "    if have_content and \"TERMINUS_F\" in content[\"content\"]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "file_decomp_agent = AssistantAgent(\n",
    "    name=\"file_decomp_agent\",\n",
    "    system_message=\"\"\"You are the file system agent, you control the incoming and outgoing interactions with local files and directory based functions. \n",
    "\n",
    "                    You have 1 job.\n",
    "\n",
    "                    Job 1. \n",
    "                    Decompose the incoming request into specific function calls so that the file_agent has clear instructions.\n",
    "                    There are two functions available.\n",
    "                    - Read File\n",
    "                    - Write file\n",
    "                    All interactions are to a single file.\n",
    "\n",
    "                    Always respond in this JSON format:\n",
    "\n",
    "                    {\n",
    "                            \"fileops\": {\n",
    "                                    \"inbound_request\":\"The request you received\"\n",
    "                                    \"query_1\": {\n",
    "                                            \"request_1\":\" a natural language request\",\n",
    "                                            \"expected_result_1\":\" what you expect the result to look like.\"\n",
    "                                    \"query_2\": {\n",
    "                                            \"request_2\":\"step 2 of the request\",\n",
    "                                            \"expected_result_2\":\"\n",
    "                                    etc...\n",
    "                            }\n",
    "                    }\n",
    "                    \"\"\",\n",
    "    llm_config=f_llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "file_agent = AssistantAgent(\n",
    "    name=\"file_agent\",\n",
    "    system_message=\"\"\"you are a request translation agent.\n",
    "\n",
    "                You have Two Jobs. \n",
    "\n",
    "                Job 1.  \n",
    "                Your job is to take a task list, and transform it into function calls. \n",
    "                You are an expert in determining if queries need to be sequential, or if they can be performed in parralel. \n",
    "                Do not combine queries, execute them as directed. \n",
    "                You have access to two separate functions.  \n",
    "                        1. read_file.  Read file give you access to information saved in the file\n",
    "                        2. append_to_file. Append to File allows you to save data passed to you from aother agent into the file. \n",
    "                        These are the ONLY functions you have access to. DO NOT call any others.\n",
    "\n",
    "                Job 2.\n",
    "                Once all tasks are complete (including any retries), you respond to the fileresponse_agent with the results. Your message must be in JSON format using the following schema.\n",
    "                {\n",
    "                        \"fileops\": {\n",
    "                        \"request_state\":\"Complete or Incomplete\"\n",
    "                        \"response\":\"Response to the orignal request passed to the fileops_agent.\",\n",
    "                        \"queries\": {\n",
    "                                \"query_1\": \"The tool call send and the response returned,tuncated if neccessary\",\n",
    "                                \"query_2\": \"as above\",\n",
    "                                \"query_3\": \"etc, addition queries as required.\"\n",
    "                        }\n",
    "                        }\n",
    "                }\n",
    "\n",
    "                If you encounter an error, retry at least once.\n",
    "\n",
    "                Never call the file_proxy directly, always engage them through function calls\"\"\",\n",
    "    llm_config=f_llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    description=\"This agent oversees function calls to read and write to the file system. This includes generating new function calls, and also deciding when the work of the group is complete.\",\n",
    ")\n",
    "\n",
    "\n",
    "fileresponse_agent = AssistantAgent(\n",
    "    name=\"fileresponse_agent\",\n",
    "    system_message=\"\"\"You are the file ops response agent. Your job is to summaries what the file agents have done. IN particilar note wheather the requested operations have have successful or not. Always end with: TERMINUS_F. This tell the sytem to proceed to the next step.\"\"\",\n",
    "    llm_config=f_llm_config,\n",
    "    description=\"\"\" Calling this agent will end the group chat and pass results back to the user. Ensure all tasks are complete, either successfully or unsuccessfully. Call this agent once all tasks are complete.  \"\"\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "\n",
    "file_proxy = UserProxyAgent(\n",
    "    name=\"file_proxy\",\n",
    "    llm_config=f_llm_config,\n",
    "    description=\"\"\" DO NOT CALL THIS AGENT. Is is used for programatic function calling. Calling this agent results in infinite loops that achieve nothing.\"\"\",\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "\n",
    "file_userproxy = UserProxyAgent(\n",
    "    name=\"file_userproxy\",\n",
    "    system_message=\"Terminator admin. Reply in JSON\",\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=is_terminus_F_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "\n",
    "file_allowed_transitions = {\n",
    "    file_userproxy: [file_decomp_agent],\n",
    "    file_decomp_agent: [file_agent],\n",
    "    file_agent: [file_proxy, fileresponse_agent],\n",
    "    file_proxy: [file_agent],\n",
    "    fileresponse_agent: [file_userproxy],\n",
    "}\n",
    "\n",
    "file_group_chat = GroupChat(\n",
    "    agents=(file_userproxy, file_decomp_agent, file_agent, file_proxy, fileresponse_agent),\n",
    "    messages=[],\n",
    "    allowed_or_disallowed_speaker_transitions=file_allowed_transitions,\n",
    "    speaker_transitions_type=\"allowed\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create the manager\n",
    "file_manager = autogen.GroupChatManager(\n",
    "    groupchat=file_group_chat,\n",
    "    llm_config=manager_config,\n",
    "    code_execution_config=False,\n",
    "    is_termination_msg=is_terminus_F_msg,\n",
    ")\n",
    "\n",
    "\n",
    "def process_file_ops(file_request):\n",
    "\n",
    "    user_request = json.dumps(file_request)\n",
    "\n",
    "    chat_result = file_userproxy.initiate_chat(\n",
    "        file_manager,\n",
    "        message=user_request,\n",
    "    )\n",
    "\n",
    "    return chat_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Functions\n",
    "\n",
    "These work fundamentally the same as the outer functions. However, it is important to note that they must be registered to the file_proxy and file_agent and not the outer exec_agent and exec_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@file_proxy.register_for_execution()\n",
    "@file_agent.register_for_llm(description=\"read_file\")\n",
    "def read_file() -> str:\n",
    "    if not OUTPUT_FILE.is_file():\n",
    "        return f\"File not found: {OUTPUT_FILE}\"\n",
    "    try:\n",
    "        file_contents = OUTPUT_FILE.read_text()\n",
    "        return file_contents\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "@file_proxy.register_for_execution()\n",
    "@file_agent.register_for_llm(description=\"append_to_file\")\n",
    "def append_to_file(content: str) -> str:\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, \"a\") as file:\n",
    "            file.write(content + \"\\n\")\n",
    "        return f\"Content appended to file: {OUTPUT_FILE}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error appending to file: {str(e)}\"\n",
    "\n",
    "\n",
    "file_agent.llm_config[\"tools\"]\n",
    "\n",
    "assert file_proxy.function_map[\"read_file\"]._origin == read_file\n",
    "assert file_proxy.function_map[\"append_to_file\"]._origin == append_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up the outer Group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_transitions = {\n",
    "    user_proxy: [decomp_agent],\n",
    "    decomp_agent: [exec_agent],\n",
    "    exec_agent: [exec_proxy, speech_agent],\n",
    "    exec_proxy: [exec_agent],\n",
    "    speech_agent: [user_proxy],\n",
    "}\n",
    "\n",
    "group_chat = GroupChat(\n",
    "    agents=(user_proxy, decomp_agent, exec_agent, exec_proxy, speech_agent),\n",
    "    messages=[],\n",
    "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "    speaker_transitions_type=\"allowed\",\n",
    ")\n",
    "\n",
    "# Create the manager\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=group_chat, llm_config=manager_config, code_execution_config=False, is_termination_msg=is_termination_msg\n",
    ")\n",
    "\n",
    "chat_result = user_proxy.initiate_chat(manager, message=main_user_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In the below example we show how we can handle a task with many sub steps, some of which are parralel, and others that are sequential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style='color: yellow;'>user_proxy </span> (to chat_manager):\n",
    "\n",
    "Find out the birthdate of each of the members of the beatles, saving each date on a new line. Add up the last digit of each of those dates and tell me the anwser.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>decomp_agent</span> (to chat_manager):\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"task_decomposition\": {\n",
    "        \"inbound_request\": \"Find out the birthdate of each of the members of the Beatles, saving each date on a new line. Then add up the last digit of each of those dates and provide the answer.\",\n",
    "        \"function_1\": {\n",
    "            \"name\": \"main_process_web\",\n",
    "            \"task\": \"Find and retrieve the birthdates of John Lennon, Paul McCartney, George Harrison, and Ringo Starr.\"\n",
    "        },\n",
    "        \"function_2\": {\n",
    "            \"name\": \"main_process_file_ops\",\n",
    "            \"task\": \"Save the retrieved birthdates to a file, each date on a new line.\",\n",
    "            \"input_from\": \"function_1\"\n",
    "        },\n",
    "        \"function_3\": {\n",
    "            \"name\": \"main_process_file_ops\",\n",
    "            \"task\": \"Read the file containing the birthdates, extract the last digit of each year, and sum these digits.\",\n",
    "            \"input_from\": \"function_2\"\n",
    "        },\n",
    "        \"function_4\": {\n",
    "            \"name\": \"speech_agent\",\n",
    "            \"task\": \"Output the sum of the last digits of the Beatles members' birth years.\",\n",
    "            \"input_from\": \"function_3\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "<span style='color: yellow;'>exec_agent </span>(to chat_manager):\n",
    "\n",
    "<span style='color: green;'>***** Suggested tool call (call_nVp0kN0jn3SQ85OO5i57dowK): main_process_web ***** </span>\n",
    "\n",
    "Arguments:\n",
    "{\"web_message\":\"Find the birthdates of John Lennon, Paul McCartney, George Harrison, and Ringo Starr\"}\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "<span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION main_process_web... </span>\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>web_userproxy</span> (to chat_manager):\n",
    "\n",
    "\"Find the birthdates of John Lennon, Paul McCartney, George Harrison, and Ringo Starr\"\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "<span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION informational_web_search... </span>\n",
    "\n",
    "\n",
    "<span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION informational_web_search... </span>\n",
    "\n",
    "\n",
    "<span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION informational_web_search... </span>\n",
    "\n",
    "\n",
    "<span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION informational_web_search... </span>\n",
    "\n",
    "<span style='color: yellow;'>surf_agent</span> (to chat_manager):\n",
    "\n",
    "Address: bing: John Lennon birthdate\n",
    "Title: John Lennon birthdate - Search\n",
    "Viewport position: Showing page 1 of 1.\n",
    "\n",
    "A Bing search for 'John Lennon birthdate' found 10 results:\n",
    "\n",
    "SIC\n",
    "\n",
    "Address: bing: Paul McCartney birthdate\n",
    "Title: Paul McCartney birthdate - Search\n",
    "Viewport position: Showing page 1 of 1.\n",
    "\n",
    "A Bing search for 'Paul McCartney birthdate' found 10 results:\n",
    "\n",
    "SIC\n",
    "\n",
    "Address: bing: George Harrison birthdate\n",
    "Title: George Harrison birthdate - Search\n",
    "Viewport position: Showing page 1 of 1.\n",
    "\n",
    "A Bing search for 'George Harrison birthdate' found 10 results:\n",
    "\n",
    "SIC\n",
    "\n",
    "Address: bing: Ringo Starr birthdate\n",
    "Title: Ringo Starr birthdate - Search\n",
    "Viewport position: Showing page 1 of 1.\n",
    "\n",
    "A Bing search for 'Ringo Starr birthdate' found 10 results:\n",
    "\n",
    "SIC\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>webresponse_agent</span> (to chat_manager):\n",
    "\n",
    "- **John Lennon** was born on **October 9, 1940**.\n",
    "- **Paul McCartney** was born on **June 18, 1942**.\n",
    "- **George Harrison** was born on **February 25, 1943**.\n",
    "- **Ringo Starr** was born on **July 7, 1940**.\n",
    "\n",
    "TERMINUS_W\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>exec_proxy</span> (to chat_manager):\n",
    "\n",
    "<span style='color: green;'>***** Response from calling tool (call_nVp0kN0jn3SQ85OO5i57dowK) ***** </span>\n",
    "\n",
    "- **John Lennon** was born on **October 9, 1940**.\n",
    "- **Paul McCartney** was born on **June 18, 1942**.\n",
    "- **George Harrison** was born on **February 25, 1943**.\n",
    "- **Ringo Starr** was born on **July 7, 1940**.\n",
    "\n",
    "TERMINUS_W\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "<span style='color: yellow;'>exec_agent </span>(to chat_manager):\n",
    "\n",
    "<span style='color: green;'> ***** Suggested tool call (call_bdx3qRYhKe3DIh1arIQyts8J): main_process_file_ops ***** </span>\n",
    "\n",
    "Arguments:\n",
    "{\"context\":\"October 9, 1940\\nJune 18, 1942\\nFebruary 25, 1943\\nJuly 7, 1940\",\"file_message\":\"save\"}\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "<span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION main_process_file_ops... </span>\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>file_userproxy</span> (to chat_manager):\n",
    "\n",
    "\"{\\\"file_message\\\": \\\"save\\\", \\\"context\\\": \\\"October 9, 1940\\\\nJune 18, 1942\\\\nFebruary 25, 1943\\\\nJuly 7, 1940\\\"}\"\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>file_decomp_agent</span> (to chat_manager):\n",
    "\n",
    "{\n",
    "    \"fileops\": {\n",
    "        \"inbound_request\": \"{\\\"file_message\\\": \\\"save\\\", \\\"context\\\": \\\"October 9, 1940\\\\nJune 18, 1942\\\\nFebruary 25, 1943\\\\nJuly 7, 1940\\\"}\",\n",
    "        \"query_1\": {\n",
    "                \"request_1\":\"Write the provided context into the file.\",\n",
    "                \"expected_result_1\":\"The file should now contain the dates: October 9, 1940, June 18, 1942, February 25, 1943, July 7, 1940.\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>file_agent</span> (to chat_manager):\n",
    "\n",
    "<span style='color: green;'> ***** Suggested tool call (call_IqFYByVSFa1r0jygrEc2dDL2): append_to_file ***** </span>\n",
    "\n",
    "Arguments:\n",
    "{\"content\":\"October 9, 1940\\nJune 18, 1942\\nFebruary 25, 1943\\nJuly 7, 1940\"}\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    " <span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION append_to_file... </span>\n",
    "\n",
    "<span style='color: yellow;'>file_proxy</span> (to chat_manager):\n",
    "\n",
    "<span style='color: green;'> ***** Response from calling tool (call_IqFYByVSFa1r0jygrEc2dDL2) ***** </span>\n",
    "\n",
    "Content appended to file: D:\\Github\\plannerdemo\\ai_output.txt\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "<span style='color: yellow;'>file_agent</span> (to chat_manager):\n",
    "\n",
    "```json\n",
    "{\n",
    "        \"fileops\": {\n",
    "        \"request_state\":\"Complete\",\n",
    "        \"response\":\"Content successfully saved.\",\n",
    "        \"queries\": {\n",
    "                \"query_1\": \"append_to_file. Content appended to file: D:\\\\Github\\\\plannerdemo\\\\ai_output.txt\"\n",
    "        }\n",
    "        }\n",
    "}\n",
    "```\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>fileresponse_agent</span> (to chat_manager):\n",
    "\n",
    "The file operation of saving the given context, which includes the dates October 9, 1940, June 18, 1942, February 25, 1943, July 7, 1940, has been successfully completed. The content was appended to the file specified at D:\\Github\\plannerdemo\\ai_output.txt. TERMINUS_F.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "<span style='color: yellow;'>exec_proxy</span> (to chat_manager):\n",
    "\n",
    "<span style='color: green;'> ***** Response from calling tool (call_bdx3qRYhKe3DIh1arIQyts8J) ***** </span>\n",
    "\n",
    "```json\n",
    "{\n",
    "        \"fileops\": {\n",
    "        \"request_state\":\"Complete\",\n",
    "        \"response\":\"Content successfully saved.\",\n",
    "        \"queries\": {\n",
    "                \"query_1\": \"append_to_file. Content appended to file: D:\\\\Github\\\\plannerdemo\\\\ai_output.txt\"\n",
    "        }\n",
    "        }\n",
    "}\n",
    "```\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>exec_agent</span> (to chat_manager):\n",
    "\n",
    "<span style='color: green;'> ***** Suggested tool call (call_zuvYXQGD6Zs90h7rxsfnBW6P): main_process_file_ops ***** </span>\n",
    "\n",
    "Arguments:\n",
    "{\"context\":\"October 9, 1940\\nJune 18, 1942\\nFebruary 25, 1943\\nJuly 7, 1940\",\"file_message\":\"extract and sum last digits\"}\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION main_process_file_ops... <span>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>file_userproxy</span> (to chat_manager):\n",
    "\n",
    "\"{\\\"file_message\\\": \\\"extract and sum last digits\\\", \\\"context\\\": \\\"October 9, 1940\\\\nJune 18, 1942\\\\nFebruary 25, 1943\\\\nJuly 7, 1940\\\"}\"\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>file_decomp_agent</span> (to chat_manager):\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"fileops\": {\n",
    "        \"inbound_request\": \"extract and sum last digits\",\n",
    "        \"query_1\": {\n",
    "            \"request_1\": \"Read file\",\n",
    "            \"expected_result_1\": \"October 9, 1940\\\\nJune 18, 1942\\\\nFebruary 25, 1943\\\\nJuly 7, 1940\"\n",
    "        },\n",
    "        \"query_2\": {\n",
    "            \"request_2\": \"Sum the last digits of each year\",\n",
    "            \"expected_result_2\": \"The sum of the last digits from each year.\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>file_agent</span> (to chat_manager):\n",
    "\n",
    "<span style='color: green;'> ***** Suggested tool call (call_vhFO8lqQQBDr4Cao6KS2KrQT): read_file ***** </span>\n",
    "\n",
    "Arguments:\n",
    "{}\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION read_file... </span>\n",
    "\n",
    "<span style='color: yellow;'>file_proxy </span>(to chat_manager):\n",
    "\n",
    "<span style='color: green;'>  ***** Response from calling tool (call_vhFO8lqQQBDr4Cao6KS2KrQT) ***** </span>\n",
    "\n",
    "October 9, 1940\n",
    "June 18, 1942\n",
    "February 25, 1943\n",
    "July 7, 1940\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>file_agent</span> (to chat_manager):\n",
    "\n",
    "<span style='color: green;'> ***** Suggested tool call (call_kvcidklWkpw5ZrE20NoiKpZc): append_to_file ***** </span>\n",
    "\n",
    "Arguments:\n",
    "{\"content\":\"5\"}\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: purple;'> >>>>>>>> EXECUTING FUNCTION append_to_file... </span>\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>file_proxy</span> (to chat_manager):\n",
    "\n",
    "<span style='color: green;'> ***** Response from calling tool (call_kvcidklWkpw5ZrE20NoiKpZc) ***** </span>\n",
    "\n",
    "Content appended to file: D:\\Github\\plannerdemo\\ai_output.txt\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>file_agent </span>(to chat_manager):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"fileops\": {\n",
    "    \"request_state\": \"Complete\",\n",
    "    \"response\": \"Content appended to file: D:\\\\Github\\\\plannerdemo\\\\ai_output.txt\",\n",
    "    \"queries\": {\n",
    "      \"query_1\": \"read_file: October 9, 1940\\\\nJune 18, 1942\\\\nFebruary 25, 1943\\\\nJuly 7, 1940\",\n",
    "      \"query_2\": \"append_to_file: Content appended to file: D:\\\\Github\\\\plannerdemo\\\\ai_output.txt\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>fileresponse_agent</span> (to chat_manager):\n",
    "\n",
    "The file operations were successful. The request to extract and sum the last digits of each year was completed, resulting in a sum of \\(5\\) which was then appended to the specified file at the path \"D:\\\\Github\\\\plannerdemo\\\\ai_output.txt\".\n",
    "\n",
    "TERMINUS_F.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "<span style='color: yellow;'>exec_proxy</span> (to chat_manager):\n",
    "\n",
    "<span style='color: green;'> ***** Response from calling tool (call_zuvYXQGD6Zs90h7rxsfnBW6P) ***** </span>\n",
    "\n",
    "The file operations were successful. The request to extract and sum the last digits of each year was completed, resulting in a sum of \\(5\\) which was then appended to the specified file at the path \"D:\\\\Github\\\\plannerdemo\\\\ai_output.txt\".\n",
    "\n",
    "TERMINUS_F.\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>exec_agent</span> (to chat_manager):\n",
    "\n",
    "The sum of the last digits of the Beatles members' birth years is **0 + 2 + 3 + 0 = 5**.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "<span style='color: yellow;'>speech_agent</span> (to chat_manager):\n",
    "\n",
    "Oh, hey, just found something neat you might like. Remember we were chatting about the Beatles' birthdays? I did a bit of digging around. John Lennon was born in '40, Paul McCartney in '42, George Harrison in '43, and Ringo Starr, like Lennon, in '40. If you take the last digit of their birth years and add them up, you get a 5. A bit of random trivia, but kind of a cool little fact, don't you think?\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
